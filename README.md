# deep-label

## Introduction

> **Derek's Notes:** I've been dreaming of an IIIIF, "Intelligent IIIF" framework, one which can deeply analyze and describe images at various scales. This Opus 4.5 vibe-coded proof-of-concept is a step in that direction- a generalized algorithm for recursive, multi-scale discovery & verification with "attention" passing spatially-attributed context at smaller scales.
> 
> One of my sub-goals is to browse all the "cats" in all artworks in a collection. The object "cutouts" generated by **deep-label** could form the basis of a fun semantic search UI, or even to train specialized image models.
>
> _TODO: Record & utilize multi-level visual descriptions._

**deep-label** is an intelligent, agentic computer vision workflow designed to analyze complex artwork with exhaustive detail.

Unlike standard object detection models that only find the most obvious elements (e.g., "person," "dog"), this system uses an **N-level recursive detection** approach with **per-tile verification** to force LLMs to look deeper, identifying specific background details, individual crowd members, and subtle narrative elements.

It then uses this exhaustive data as "Ground Truth" to generate high-quality, hallucination-free accessibility descriptions (Alt Text and Long Descriptions) for museum contexts.

_Example Deep-label segmentation of "Hunting near Hartenfels Castle":_

![Deep-label segmentation of "Hunting near Hartenfels Castle"](docs/images/hunting-scene.jpg)

_Example cutouts generated for "The Fall of the Magician":_

![Example of cutouts for "The Fall of the Magician"](docs/images/cutouts.jpg)

---

## The Problem

### 1. The Metadata Gap
Museums index titles, not pixels. Search for "dog" and you get paintings *named* "dog" — not the thousands where a dog appears in the background. **Deep-label turns visual content into searchable data.**

### 2. The Domain Shift Problem
Standard vision models (YOLO, COCO) are trained on photographs. They see brushstrokes as noise and stylization as error. A Cubist face registers as not-a-face. **This pipeline focuses on structure over style** — recognizing a "person" whether rendered by Vermeer or Picasso.

### 3. Distant Viewing
Art history is traditionally qualitative — a scholar manually studies hundreds of works in a career. **Automated labeling enables macro-analysis**: track how "hands" evolved from Medieval to Renaissance across 100,000 images.

### 4. Accessibility at Scale
Alt-text for museum collections is scarce and expensive to produce manually. **Deep-label generates rich, hierarchical descriptions automatically** — enabling screen readers, audio tours, and region-based exploration for visually impaired users.

**In short:** Current AI sees art as broken photos. Current museum databases are blind to their own visual content. This project bridges both gaps.

### Why Existing Tools Fail

Consider a [complex hunting scene](https://www.clevelandart.org/art/1958.425) with dozens of hounds. Here's what happens when you try existing approaches:

| Approach | Result |
|----------|--------|
| ![SAM 3 segmentation](docs/images/sam3.jpg) | **Segmentation models** like [Meta SAM 3](https://ai.meta.com/sam3/) excel at pixel-perfect boundaries but often lack the semantic flexibility for stylized art. They struggle to map specific iconography (e.g., "demon") to general classes they were trained on (e.g., "person"). While SAM 3 improves on this, it still suffers from low recall in dense scenes, identifying only 15 "hounds" here where dozens exist. |
| ![NanoBanana segmentation](docs/images/nanobanana.jpg) | **Image generators** like [Nano Banana Pro](https://gemini.google/overview/image-generation/) can be asked to visually "highlight the hounds." While useful for general localization, generative models prioritize global image coherence over exhaustive extraction. As a result, it highlights the most obvious clusters but misses individual outlying instances. |
| ![Gemini 3 direct prompting](docs/images/gemini3.jpg) | **VLM direct prompting**: Asking [Gemini 3](https://gemini.google.com/) for direct bounding box coordinates works well on simple photos but suffers from coordinate drift in complex art. The model correctly perceives the density (returning ~36 instances), but lacks the precision to ground them accurately, resulting in "floating" boxes that don't align with the pixels. |

**deep-label** solves this with recursive multi-scale detection, per-tile verification, and intelligent segmentation strategies to achieve exhaustive coverage without hallucination.

---

## How It Works

The recursive detector uses a true N-level approach where the LLM decides at each tile whether to detect objects directly or zoom in further for more detail.

### Phase 1: Recursive Discovery

At each tile (starting with the full image), the detector:

1. **Analyzes the scene** — identifies what kinds of objects are present
2. **Decides actions per kind** — for each object type, chooses a strategy:
   - `detect_individual` — box every visible instance
   - `detect_representative` — box 3-8 diverse examples (for crowds, forests)
   - `detect_region` — box areas/masses (e.g., "forest" as a whole)
   - `zoom_in` — objects too small at this scale, recurse into quadrants
   - `ignore` — not worth detecting (texture, background)
3. **Passes context downward** — child tiles receive the parent's scene description to maintain coherence
4. **Recurses adaptively** — only splits into quadrants when `zoom_in` is requested

This continues until max depth is reached or no further zooming is needed.

### Phase 2: Per-Tile Verification

For each detection within a tile:

1. **Initial detection** — LLM draws bounding boxes for requested kinds
2. **Verification overlay** — numbered boxes are rendered on the tile
3. **LLM review** — verifies each box: remove wrong ones, correct positions, add missing
4. **Iterate** — repeats until stable (configurable rounds, default: 2)

This catches hallucinations and coordinate drift at the tile level, where the LLM can actually see the objects clearly.

### Phase 3: Global Post-Processing

After all recursion completes:

- **Deduplication** — merges overlapping boxes using IoU threshold
- **Alias preservation** — different labels for same object kept as `aliases`
- **Importance scoring** — ranks objects by size, centrality, and rarity

### Phase 4: Output Generation

- **Descriptions**: Alt text (10-18 words) + long description (150-200 words) using verified object list as ground truth
- **Cutouts**: Cropped images for each detected object with configurable padding
- **Annotated image**: Bounding box overlay visualization

---

## Segmentation Strategies

The LLM intelligently selects how to detect each object type:

| Strategy | When Used | Example |
|----------|-----------|---------|
| `detect_individual` | Countable, distinct objects | Each horse in a cavalry |
| `detect_representative` | Many similar objects, spatially distributed | Sample trees in a forest |
| `detect_region` | Mass/area, not individual items | A crowd, a lake, storm clouds |
| `zoom_in` | Objects too small to box accurately | Tiny figures in a vast landscape |
| `ignore` | Texture/background, not worth boxing | Grass texture, sky gradient |

---

## Getting Started

### 1. Installation
```bash
npm install
```

### 2. Environment Setup
Create a `.env` file:
```env
GOOGLE_GENERATIVE_AI_API_KEY=your_api_key_here
```

### 3. Add a New Artwork

**Step 1:** Create a directory using a URL-friendly slug:
```bash
mkdir -p public/artworks/my-painting
```

**Step 2:** Add your image (must be named `image.jpg`, `image.jpeg`, or `image.png`):
```bash
cp path/to/your-image.jpg public/artworks/my-painting/image.jpg
```

**Step 3 (optional):** Add metadata in `artwork.json`:
```bash
cat > public/artworks/my-painting/artwork.json << 'EOF'
{
  "title": "My Painting Title",
  "artist": "Artist Name",
  "date": "1850",
  "medium": "Oil on canvas",
  "url": "https://museum.org/artwork/12345"
}
EOF
```

If omitted, the title defaults to the slug (e.g., "my-painting" → "My Painting").

**Step 4:** Run the detector:
```bash
npm run detect my-painting
```

This generates in the artwork directory:
- `detected_objects.json` — full detection payload
- `cutouts/` — cropped images per object
- `annotated.png` — visualization with boxes

Use `--force` to overwrite existing outputs.

### 4. View Results
```bash
npm run dev
```
Open http://localhost:3000 for an interactive zoomable viewer.

---

## CLI Reference

```
npm run detect <slug> [options]
npx tsx scripts/detect-recursive.ts <slug> [options]
npx tsx scripts/detect-recursive.ts -i <image> -o <output> [options]

Recursion Options:
  --max-depth <n>           Maximum recursion depth (default: 3)
  --min-tile-size <px>      Minimum tile size in pixels (default: 256)
  --verify-rounds <n>       Verification rounds per detection (default: 2)
  --concurrency <n>         Max concurrent API calls (default: 6)

Output Options:
  --force                   Overwrite existing output files
  --no-cutouts              Disable cutout generation
  --cutouts-format <fmt>    webp or png (default: webp)
  --cutouts-padding <pct>   Padding as decimal (default: 0.10)
  --cutouts-max <n>         Max cutouts to generate (default: 100)
  --no-annotate             Skip annotated image
  --no-descriptions         Skip description generation

Model Options:
  --model <name>            Detection model (default: gemini-3-pro-preview)
  --description-model <n>   Model for descriptions (default: same as --model)

  -h, --help                Show all options
```

**Environment variables:** Options have env var equivalents (e.g., `MAX_DEPTH`, `VERIFY_ROUNDS`, `CONCURRENCY`).

### Original Detector

The original 2-level detector is preserved for comparison:

```bash
npm run detect:original <slug> [options]
```

See `src/lib/detector-original.ts` for implementation.

---

## Performance Tuning

The recursive detector trades thoroughness for speed. For faster runs:

| Option | Effect | Tradeoff |
|--------|--------|----------|
| `--max-depth 2` | Fewer tiles (21 vs 85) | May miss tiny objects |
| `--verify-rounds 1` | Halves verification calls | Slightly lower accuracy |
| `--verify-rounds 0` | No verification | Fast but more errors |
| `--no-cutouts` | Skip cutout generation | No cropped images |
| `--min-tile-size 512` | Larger minimum tiles | Less detail at edges |

A typical run with defaults (depth 3, verify 2) makes ~70-100 API calls. With `--max-depth 2 --verify-rounds 1`, this drops to ~20-30 calls.

---

## Data Format

Output `detected_objects.json`:

```json
{
  "strategy": "recursive-v2",
  "image_path": "public/artworks/my-painting/image.jpg",
  "model_name": "gemini-3-pro-preview",
  "max_depth": 3,
  "min_tile_size": 256,
  "verify_rounds": 2,
  "objects": [
    {
      "label": "hound",
      "type": "animal",
      "box_2d": [80, 640, 155, 780],
      "importance": 0.42,
      "importance_geom": 0.42,
      "importance_rank": 5,
      "depth": 2
    }
  ],
  "descriptions": {
    "alt_text": "A hunting party with riders and hounds chases a stag...",
    "long_description": "..."
  },
  "cutouts": {
    "enabled": true,
    "count": 45,
    "format": "webp"
  }
}
```

**Coordinates:** `box_2d` is `[xmin, ymin, xmax, ymax]` normalized to `[0, 1000]`.

**Depth:** Indicates at which recursion level the object was detected (0 = full image).

---

## Tech Stack

- **Runtime:** Node.js / TypeScript
- **AI Framework:** [Vercel AI SDK](https://sdk.vercel.ai/docs) (`generateObject`)
- **Model Provider:** Google Generative AI (Gemini)
- **Validation:** Zod schema enforcement
- **Image Processing:** sharp (tiling, cutouts, SVG overlay)
- **Web Viewer:** Next.js + OpenSeadragon

---

## Project Structure

```
scripts/
  detect-recursive.ts   # Main detection CLI (recursive)
  detect.ts             # Original detector CLI (preserved)
src/
  lib/
    recursive_detector.ts  # Core recursive detection pipeline
    detector-original.ts   # Original 2-level detector (preserved)
    detection-utils.ts     # Shared utilities (geometry, dedup, scoring)
    annotator.ts           # Image annotation
    pool.ts                # AI request pooling with rate limiting
  app/                     # Next.js web viewer
public/artworks/           # Artwork data (per-slug directories)
```
